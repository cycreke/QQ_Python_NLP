{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-a115efd7ec5a>:87: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Accuracy at 0: 73.43750\n",
      "Accuracy at 100: 100.00000\n",
      "Accuracy at 200: 100.00000\n",
      "Accuracy at 300: 100.00000\n",
      "Accuracy at 400: 100.00000\n",
      "Accuracy at 500: 100.00000\n",
      "Accuracy at 600: 100.00000\n",
      "Accuracy at 700: 100.00000\n",
      "Accuracy at 800: 100.00000\n",
      "Accuracy at 900: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 128;embedding_dimension = 64;num_classes = 2\n",
    "hidden_layer_size = 32;times_steps = 6;element_size = 1\n",
    "\n",
    "digit_to_word_map = {1:\"One\",2:\"Two\", 3:\"Three\", 4:\"Four\", 5:\"Five\",6:\"Six\",7:\"Seven\",8:\"Eight\",9:\"Nine\"}\n",
    "digit_to_word_map[0] = \"PAD\"\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2), rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2),rand_seq_len)\n",
    "    if rand_seq_len<6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints, [0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints, [0]*(6-rand_seq_len))\n",
    "    even_sentences.append(\" \".join([digit_to_word_map[r] for r in rand_odd_ints]))\n",
    "    odd_sentences.append(\" \".join([digit_to_word_map[r] for r in rand_even_ints]))\n",
    "\n",
    "data = even_sentences+odd_sentences\n",
    "# Same seq lengths for even, odd sentences\n",
    "seqlens*=2\n",
    "\n",
    "even_sentences[0:6]\n",
    "\n",
    "word2index_map ={}\n",
    "index=0\n",
    "for sent in data:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "# Inverse map\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)\n",
    "\n",
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding\n",
    "\n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices) #shuffle\n",
    "data = np.array(data)[data_indices]\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]\n",
    "\n",
    "def get_sentence_batch(batch_size,data_x, data_y,data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].lower().split()] for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x,y,seqlens\n",
    "\n",
    "_inputs = tf.placeholder(tf.int32, shape=[batch_size,times_steps])\n",
    "_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "# seqlens for dynamic calculation\n",
    "_seqlens = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "\n",
    "with tf.name_scope(\"embeddings\"):\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_dimension],-1.0, 1.0),name='embedding')\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "\n",
    "\n",
    "#LSTM\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size,forget_bias=1.0)\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, embed,sequence_length = _seqlens, dtype=tf.float32)\n",
    "    weights = { 'linear_layer': tf.Variable(tf.truncated_normal([hidden_layer_size,num_classes],mean=0,stddev=.01))}\n",
    "    biases ={'linear_layer':tf.Variable(tf.truncated_normal([num_classes],mean=0,stddev=.01))}\n",
    "    # Extract the last relevant output and use in a linear layer\n",
    "    final_output = tf.matmul(states[1],weights[\"linear_layer\"]) + biases[\"linear_layer\"]\n",
    "    softmax = tf.nn.softmax_cross_entropy_with_logits(logits = final_output,labels = _labels)\n",
    "    cross_entropy = tf.reduce_mean(softmax)\n",
    "    \n",
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(_labels,1), tf.argmax(final_output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction,tf.float32)))*100\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch,seqlen_batch = get_sentence_batch(batch_size,train_x,train_y,train_seqlens)\n",
    "        sess.run(train_step,feed_dict={_inputs:x_batch, _labels:y_batch, _seqlens:seqlen_batch})\n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={_inputs:x_batch,_labels:y_batch,_seqlens:seqlen_batch})\n",
    "            print(\"Accuracy at %d: %.5f\" % (step, acc))\n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test,seqlen_test = get_sentence_batch(batch_size,test_x,test_y,test_seqlens)\n",
    "        batch_pred,batch_acc = sess.run([tf.argmax(final_output,1),accuracy],\n",
    "        feed_dict={_inputs:x_test,_labels:y_test,_seqlens:seqlen_test})\n",
    "        print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))\n",
    "    output_example = sess.run([outputs],feed_dict={_inputs:x_test,_labels:y_test,_seqlens:seqlen_test})\n",
    "    states_example = sess.run([states[1]],feed_dict={_inputs:x_test,_labels:y_test,_seqlens:seqlen_test})\n",
    "    states_example1 = sess.run([states],feed_dict={_inputs:x_test,_labels:y_test,_seqlens:seqlen_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27944392, -0.54158884, -0.58350223,  0.4804586 , -0.51977104,\n",
       "        -0.4177508 ,  0.3554054 , -0.45807445, -0.52155864,  0.52196264,\n",
       "         0.47510675, -0.45088238,  0.45196566, -0.4896564 , -0.51467115,\n",
       "        -0.40138274,  0.614623  , -0.40318283,  0.2357543 , -0.32615232,\n",
       "        -0.48341882,  0.377625  , -0.40499815,  0.30868793,  0.4935338 ,\n",
       "        -0.42156386,  0.22172411, -0.14312477,  0.45683843, -0.29186282,\n",
       "        -0.46392113,  0.25222215],\n",
       "       [ 0.62624604, -0.7989841 , -0.82225066,  0.7441918 , -0.80905545,\n",
       "        -0.7405462 ,  0.6765096 , -0.7301297 , -0.8032071 ,  0.794739  ,\n",
       "         0.7637833 , -0.7749163 ,  0.73039556, -0.7661446 , -0.7774762 ,\n",
       "        -0.69653213,  0.8731955 , -0.6580318 ,  0.5824946 , -0.5979315 ,\n",
       "        -0.7542486 ,  0.74311113, -0.721142  ,  0.54478663,  0.77420825,\n",
       "        -0.7428881 ,  0.505696  , -0.29532123,  0.7522843 , -0.5963745 ,\n",
       "        -0.76621604,  0.4892354 ],\n",
       "       [ 0.8107909 , -0.78188735, -0.85633326,  0.8601144 , -0.88929516,\n",
       "        -0.89418966,  0.8425631 , -0.8062418 , -0.8487602 ,  0.8643513 ,\n",
       "         0.8388849 , -0.85885626,  0.8340177 , -0.7659282 , -0.8362075 ,\n",
       "        -0.7828185 ,  0.9136981 , -0.8002425 ,  0.58923095, -0.6867212 ,\n",
       "        -0.78932726,  0.838949  , -0.8403331 ,  0.77699786,  0.7833472 ,\n",
       "        -0.8237713 ,  0.6833953 , -0.44549048,  0.8475063 , -0.77509314,\n",
       "        -0.8529563 ,  0.53404933],\n",
       "       [ 0.8711233 , -0.86314136, -0.89489126,  0.88431305, -0.94868815,\n",
       "        -0.8856533 ,  0.88387185, -0.7834286 , -0.8911504 ,  0.87401724,\n",
       "         0.88339996, -0.9069779 ,  0.8700907 , -0.7928667 , -0.8196861 ,\n",
       "        -0.8393843 ,  0.9144197 , -0.76425576,  0.6997665 , -0.84657836,\n",
       "        -0.80744755,  0.85037273, -0.88324577,  0.78543234,  0.8679687 ,\n",
       "        -0.88679886,  0.7278372 , -0.44246083,  0.91359   , -0.8406975 ,\n",
       "        -0.9166311 ,  0.6550981 ],\n",
       "       [ 0.8142387 , -0.8806417 , -0.91636336,  0.8798984 , -0.92787087,\n",
       "        -0.8949388 ,  0.86438483, -0.85578585, -0.9186318 ,  0.9189554 ,\n",
       "         0.8891043 , -0.90847427,  0.872794  , -0.8482239 , -0.8559028 ,\n",
       "        -0.8370543 ,  0.96018445, -0.77945006,  0.76951265, -0.7635048 ,\n",
       "        -0.88074905,  0.91710716, -0.8639425 ,  0.6816904 ,  0.8832679 ,\n",
       "        -0.8614126 ,  0.6931858 , -0.34655112,  0.9030468 , -0.8130161 ,\n",
       "        -0.8998928 ,  0.563636  ],\n",
       "       [ 0.8244984 , -0.8837261 , -0.9164325 ,  0.88607794, -0.9323245 ,\n",
       "        -0.8979708 ,  0.869714  , -0.87029904, -0.92366487,  0.9232877 ,\n",
       "         0.89592713, -0.9102634 ,  0.88085425, -0.8548438 , -0.8665261 ,\n",
       "        -0.8429517 ,  0.964144  , -0.7791912 ,  0.7748821 , -0.7753577 ,\n",
       "        -0.88919324,  0.92110074, -0.87192583,  0.6893033 ,  0.8870727 ,\n",
       "        -0.87398255,  0.70223844, -0.35227987,  0.9078322 , -0.8290536 ,\n",
       "        -0.9023689 ,  0.58018273]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_example[0][1] #第2个句子的y1,y2,y3,y4,y5,y6 ,即对应的state 1 -6 的各个神经元参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8244984 , -0.8837261 , -0.9164325 ,  0.88607794, -0.9323245 ,\n",
       "       -0.8979708 ,  0.869714  , -0.87029904, -0.92366487,  0.9232877 ,\n",
       "        0.89592713, -0.9102634 ,  0.88085425, -0.8548438 , -0.8665261 ,\n",
       "       -0.8429517 ,  0.964144  , -0.7791912 ,  0.7748821 , -0.7753577 ,\n",
       "       -0.88919324,  0.92110074, -0.87192583,  0.6893033 ,  0.8870727 ,\n",
       "       -0.87398255,  0.70223844, -0.35227987,  0.9078322 , -0.8290536 ,\n",
       "       -0.9023689 ,  0.58018273], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_example[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LSTMStateTuple(c=array([[-1.4841735,  1.7360431,  1.405453 , ...,  1.5869547,  1.2460747,\n",
       "         -2.05087  ],\n",
       "        [ 3.0395713, -3.4637344, -3.472891 , ..., -2.4197836, -3.2415657,\n",
       "          2.3022718],\n",
       "        [-1.7928432,  1.775701 ,  1.8385878, ...,  1.8309517,  1.606359 ,\n",
       "         -1.9575055],\n",
       "        ...,\n",
       "        [ 2.2311816, -2.1150064, -2.3189468, ..., -2.015119 , -2.335775 ,\n",
       "          1.7197986],\n",
       "        [ 2.6387937, -2.5974407, -2.8085098, ..., -2.4167647, -2.8322644,\n",
       "          2.116045 ],\n",
       "        [ 2.6751957, -2.4151773, -2.8195524, ..., -2.3012977, -2.7054276,\n",
       "          1.9294608]], dtype=float32), h=array([[-0.77786416,  0.81522083,  0.68201816, ...,  0.8207027 ,\n",
       "          0.6731336 , -0.8942009 ],\n",
       "        [ 0.8244984 , -0.8837261 , -0.9164325 , ..., -0.8290536 ,\n",
       "         -0.9023689 ,  0.58018273],\n",
       "        [-0.7682362 ,  0.8482324 ,  0.8112685 , ...,  0.79402673,\n",
       "          0.6098539 , -0.8318191 ],\n",
       "        ...,\n",
       "        [ 0.86157584, -0.7911887 , -0.8758014 , ..., -0.8373347 ,\n",
       "         -0.8821737 ,  0.556637  ],\n",
       "        [ 0.8679533 , -0.7734428 , -0.802785  , ..., -0.8051045 ,\n",
       "         -0.85595727,  0.46971732],\n",
       "        [ 0.87509614, -0.7998379 , -0.88637537, ..., -0.85095316,\n",
       "         -0.89131665,  0.5661476 ]], dtype=float32))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_example1 #有两个隐状态c,和h， c是输入门, h是输出门\n",
    "                #state是final state，如果有n layer，则是final state也有n个元素，对应每一层的state。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-4f3550fe0d8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstates_example1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "states_example1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
