{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TFLearn\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>Function</td> <td>Description</td>\n",
    "    </tr>\n",
    "      <tr>\n",
    "<td>evaluate(X, Y, batch_size=128)</td> <td>Perform evaluations of the model on given samples.</td>\n",
    "       </tr>\n",
    "          <tr>\n",
    "<td>fit(X, Y, n_epoch=10)</td> <td>Train the model with input features X and target Y to the network.</td>\n",
    "           </tr>\n",
    "   <tr>\n",
    "<td>get_weights(weight_tensor)</td> <td>Get a variable’s weights.</td>\n",
    "       </tr>\n",
    "       <tr>\n",
    "<td>load(model_file)</td> <td>Restore model weights.</td>\n",
    "           </tr>\n",
    "       <tr>\n",
    "<td>predict(X)</td> <td>Get model predictions for the given input data.</td>\n",
    "       </tr>\n",
    "       <tr>\n",
    "<td>save(model_file)</td> <td>Save model weights.</td>\n",
    "       </tr>\n",
    "       <tr>\n",
    "<td>set_weights(tensor,weights)</td> <td>Assign a tensor variable a given value.</td>\n",
    "       </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2579  | total loss: \u001b[1m\u001b[32m0.13323\u001b[0m\u001b[0m | time: 17.469s\n",
      "| Adam | epoch: 003 | loss: 0.13323 - acc: 0.9706 -- iter: 54976/55000\n",
      "Training Step: 2580  | total loss: \u001b[1m\u001b[32m0.12498\u001b[0m\u001b[0m | time: 18.652s\n",
      "| Adam | epoch: 003 | loss: 0.12498 - acc: 0.9720 | val_loss: 0.04346 - val_acc: 0.9868 -- iter: 55000/55000\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\xcm\\Python\\QQ_PYTHON_NLP\\MNIST_tflearn_checkpoints\\checkpoint-2580 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:C:\\Users\\xcm\\Python\\QQ_PYTHON_NLP\\MNIST_tflearn_checkpoints\\checkpoint-2580 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[0.9868]\n",
      "0.9868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIn TFLearn, each iteration is a full pass (forward and backward) over one\\nexample. The training step is the number of full passes to perform, determined\\nby the batch size you set (the default is 64), and an epoch is a full pass over all\\nthe training examples (50,000 in the case of MNIST).\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CNN\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "import numpy as np\n",
    "# Data loading and basic transformations\n",
    "import tflearn.datasets.mnist as mnist\n",
    "X, Y, X_test, Y_test = mnist.load_data(one_hot=True)\n",
    "X = X.reshape([-1, 28, 28, 1])\n",
    "X_test = X_test.reshape([-1, 28, 28, 1])\n",
    "\n",
    "# Building the network\n",
    "CNN = input_data(shape=[None, 28, 28, 1], name='input')\n",
    "CNN = conv_2d(CNN, 32, 5, activation='relu', regularizer=\"L2\")\n",
    "CNN = max_pool_2d(CNN, 2)\n",
    "CNN = local_response_normalization(CNN)\n",
    "CNN = conv_2d(CNN, 64, 5, activation='relu', regularizer=\"L2\")\n",
    "CNN = max_pool_2d(CNN, 2)\n",
    "CNN = local_response_normalization(CNN)\n",
    "CNN = fully_connected(CNN, 1024, activation=None)\n",
    "CNN = dropout(CNN, 0.5)\n",
    "CNN = fully_connected(CNN, 10, activation='softmax')\n",
    "CNN = regression(CNN, optimizer='adam', learning_rate=0.0001,loss='categorical_crossentropy', name='target')\n",
    "\n",
    "# Training the network\n",
    "model = tflearn.DNN(CNN,tensorboard_verbose=0,\n",
    "                    tensorboard_dir = 'MNIST_tflearn_board',checkpoint_path = 'MNIST_tflearn_checkpoints\\checkpoint')\n",
    "model.fit({'input': X}, {'target': Y}, n_epoch=3, \n",
    "          validation_set=({'input': X_test}, {'target': Y_test}),snapshot_step=1000,show_metric=True, run_id='convnet_mnist')\n",
    "\n",
    "evaluation = model.evaluate({'input': X_test},{'target': Y_test})\n",
    "print(evaluation)\n",
    "pred = model.predict({'input': X_test})\n",
    "print((np.argmax(Y_test,1)==np.argmax(pred,1)).mean())\n",
    "                    \n",
    "                    \n",
    "'''\n",
    "In TFLearn, each iteration is a full pass (forward and backward) over one\n",
    "example. The training step is the number of full passes to perform, determined\n",
    "by the batch size you set (the default is 64), and an epoch is a full pass over all\n",
    "the training examples (50,000 in the case of MNIST).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7039  | total loss: \u001b[1m\u001b[32m0.02698\u001b[0m\u001b[0m | time: 39.781s\n",
      "| Adam | epoch: 010 | loss: 0.02698 - acc: 0.9926 -- iter: 22496/22500\n",
      "Training Step: 7040  | total loss: \u001b[1m\u001b[32m0.02610\u001b[0m\u001b[0m | time: 40.984s\n",
      "| Adam | epoch: 010 | loss: 0.02610 - acc: 0.9934 | val_loss: 0.92076 - val_acc: 0.8084 -- iter: 22500/22500\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "# IMDb dataset loading\n",
    "train, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000,valid_portion=0.1)\n",
    "X_train, Y_train = train\n",
    "X_test, Y_test = test\n",
    "'''\n",
    "by equalizing the sequences with zero-padding by using\n",
    "tflearn.data_utils.pad_sequences() and setting 100 as the maximum sequence length\n",
    "'''\n",
    "X_train = pad_sequences(X_train, maxlen=100, value=0.)\n",
    "X_test = pad_sequences(X_test, maxlen=100, value=0.)\n",
    "Y_train = to_categorical(Y_train, nb_classes=2)\n",
    "Y_test = to_categorical(Y_test, nb_classes=2)\n",
    "\n",
    "RNN = tflearn.input_data([None, 100]) #100的长度\n",
    "RNN = tflearn.embedding(RNN, input_dim=10000, output_dim=128)\n",
    "\n",
    "#LSTM\n",
    "RNN = tflearn.lstm(RNN, 128, dropout=0.8)\n",
    "RNN = tflearn.fully_connected(RNN, 2, activation='softmax')\n",
    "RNN = tflearn.regression(RNN, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy')\n",
    "# Training the network\n",
    "model = tflearn.DNN(RNN, tensorboard_verbose=0)\n",
    "model.fit(X_train, Y_train, validation_set=(X_test, Y_test), show_metric=True, batch_size=32) #n_epoch=10 默认"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
