{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'HelloWorld!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "h = tf.constant('Hello')\n",
    "w = tf.constant('World!')\n",
    "hw = h + w\n",
    "with tf.Session() as sess:\n",
    "    ans = sess.run(hw)\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\image_nano\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\image_nano\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\image_nano\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\image_nano\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-5-dc25160382e8>:12: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Accuracy: 91.72%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "DATA_DIR = 'data'\n",
    "NUM_STEPS = 1000\n",
    "MINIBATCH_SIZE = 100\n",
    "data = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "y_pred = tf.matmul(x, W)\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "logits=y_pred, labels=y_true))\n",
    "gd_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "correct_mask = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))\n",
    "with tf.Session() as sess:\n",
    "# Train\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(NUM_STEPS):\n",
    "        batch_xs, batch_ys = data.train.next_batch(MINIBATCH_SIZE)\n",
    "        sess.run(gd_step, feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "    # Test\n",
    "    ans = sess.run(accuracy, feed_dict={x: data.test.images,\n",
    "                y_true: data.test.labels})\n",
    "print(\"Accuracy: {:.4}%\".format(ans*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/graph_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>tf.add()</td><td>a + b</td><td>Adds a and b, element-wise. </td>\n",
    "        </tr>\n",
    "    <tr>\n",
    "        <td>tf.multiply()</td><td> a * b</td><td>Multiplies a and b, element-wise.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>tf.subtract()</td><td>a - b</td><td>Subtracts a from b, element-wise.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>tf.divide()</td><td>a / b</td><td>Computes Python-style division of a by b.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>tf.pow()</td><td>a ** b</td><td> Returns the result of raising each element in a to its\n",
    "corresponding element b, element-wise.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.mod()</td><td>a % b</td><td>Returns the element-wise modulo.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.logical_and()</td><td>a & b</td><td>Returns the truth table of a & b, element-wise. dtype must\n",
    "be tf.bool.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.greater()</td><td>a > b</td><td>Returns the truth table of a > b, element-wise.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.greater_equal()</td> <td>a >= b</td> <td>Returns the truth table of a >= b, element-wise.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.less_equal()</td> <td>a <= b</td> <td>Returns the truth table of a <= b, element-wise.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.less()</td> <td>a < b</td> <td>Returns the truth table of a < b, element-wise.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.negative()</td> <td>-a</td> <td>Returns the negative value of each element in a.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.logical_not()</td> <td>~a</td> <td>Returns the logical NOT of each element in a. Only\n",
    "compatible with Tensor objects with dtype of tf.bool.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.abs()</td> <td>abs(a)</td> <td>Returns the absolute value of each element in a.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.logical_or()</td> <td>a | b</td> <td>Returns the truth table of a | b, element-wise. dtype must\n",
    "be tf.bool.</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs = 5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant(5)\n",
    "b = tf.constant(2)\n",
    "c = tf.constant(3)\n",
    "d = tf.multiply(a, b)\n",
    "e = tf.add(c, b)\n",
    "f = tf.subtract(d, e)\n",
    "\n",
    "sess = tf.Session()\n",
    "output = sess.run(f)\n",
    "'''\n",
    "The execution itself is then done with the .run() method of the Session\n",
    "object. When called, this method completes one set of computations in our\n",
    "graph in the following manner: it starts at the requested output(s) and then\n",
    "works backward, computing nodes that must be executed according to the set\n",
    "of dependencies. Therefore, the part of the graph that will be computed\n",
    "depends on our output query.\n",
    "'''\n",
    "sess.close()\n",
    "print(\"outs = {}\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/graph_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs = 0.2631578947368421\n"
     ]
    }
   ],
   "source": [
    "## for A\n",
    "\n",
    "a = tf.constant(4)\n",
    "b = tf.constant(3)\n",
    "d = tf.add(a, b) # 7\n",
    "c = tf.multiply(a, b) #12\n",
    "f = tf.add(d, c) # 19\n",
    "e = tf.subtract(c, d) # 5 \n",
    "g = tf.divide(e, f) # 5/19\n",
    "\n",
    "sess = tf.Session()\n",
    "output = sess.run(g)\n",
    "sess.close()\n",
    "print(\"outs = {}\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Graph\n",
    "g1 = tf.get_default_graph()\n",
    "g2 = tf.Graph()\n",
    "print(g1 is tf.get_default_graph())\n",
    "with g2.as_default():\n",
    "    print(g1 is tf.get_default_graph())\n",
    "print(g1 is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs = [5, 2, 3, 10, 5, 5]\n",
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "#Fetches\n",
    "a = tf.constant(5)\n",
    "b = tf.constant(2)\n",
    "c = tf.constant(3)\n",
    "d = tf.multiply(a, b)\n",
    "e = tf.add(c, b)\n",
    "f = tf.subtract(d, e)\n",
    "with tf.Session() as sess:\n",
    "    fetches = [a,b,c,d,e,f]\n",
    "    outs = sess.run(fetches)\n",
    "print(\"outs = {}\".format(outs))\n",
    "print(type(outs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_21:0\", shape=(), dtype=float64)\n",
      "<dtype: 'float64'>\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "c = tf.constant(4.0, dtype=tf.float64)\n",
    "print(c)\n",
    "print(c.dtype)\n",
    "\n",
    "#To change the data type setting of a Tensor object, we can use the tf.cast() operation\n",
    "x = tf.constant([1,2,3],name='x',dtype=tf.float32)\n",
    "print(x.dtype)\n",
    "x = tf.cast(x,tf.int64)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python List input: (2, 3)\n",
      "3d NumPy array input: (2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "c = tf.constant([[1,2,3],\n",
    "                [4,5,6]])\n",
    "print(\"Python List input: {}\".format(c.get_shape())) ##2 * 3 matrix\n",
    "c = tf.constant(np.array([\n",
    "        [[1,2,3],\n",
    "        [4,5,6]],\n",
    "        [[1,1,1],\n",
    "        [2,2,2]]\n",
    "        ]))\n",
    "print(\"3d NumPy array input: {}\".format(c.get_shape()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content of 'c':\n",
      " [0. 1. 2. 3. 4.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## tf.linspace(a, b, n) that creates n evenly spaced values from a to b.\n",
    "sess = tf.InteractiveSession()\n",
    "c = tf.linspace(0.0, 4.0, 5)\n",
    "print(\"The content of 'c':\\n {}\\n\".format(c.eval()))\n",
    "sess.close()\n",
    "\n",
    "'''\n",
    "tf.InteractiveSession() allows you to replace the usual tf.Session(), so that\n",
    "you don’t need a variable holding the session for running ops. This can be\n",
    "useful in interactive Python environments, like when writing IPython\n",
    "notebooks, for instance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            TensorFlow</td> <td>operation Description</td></tr>\n",
    "    <tr>\n",
    "<td>tf.constant(value)</td> <td>Creates a tensor populated with the value or values specified\n",
    "by the argument value</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "<td>tf.fill(shape, value)</td> <td>Creates a tensor of shape shape and fills it with value</td>\n",
    "    </tr>\n",
    "            <tr>\n",
    "<td>tf.zeros(shape)</td> <td>Returns a tensor of shape shape with all elements set to 0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.zeros_like(tensor)</td> <td>Returns a tensor of the same type and shape as tensor with all\n",
    "elements set to 0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.ones(shape)</td> <td>Returns a tensor of shape shape with all elements set to 1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.ones_like(tensor)</td> <td>Returns a tensor of the same type and shape as tensor with all\n",
    "elements set to 1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.random_normal(shape,\n",
    "mean, stddev)</td>\n",
    "<td>Outputs random values from a normal distribution</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.truncated_normal(shape,\n",
    "mean, stddev)</td>\n",
    "<td>Outputs random values from a truncated normal\n",
    "distribution (values whose magnitude is more than two\n",
    "standard deviations from the mean are dropped and repicked)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.random_uniform(shape,\n",
    "minval, maxval)</td>\n",
    "<td>Generates values from a uniform distribution in the range\n",
    "[minval, maxval)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>tf.random_shuffle(tensor)</td> <td>Randomly shuffles a tensor along its first dimension</td>\n",
    "    </tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(3,)\n",
      "Before expand dim: [1 0 1]\n",
      "(3, 1)\n",
      "After expand dim: [[1]\n",
      " [0]\n",
      " [1]]\n",
      "matmul result:\n",
      " [[ 4]\n",
      " [10]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([ [1,2,3],\n",
    "[4,5,6] ])\n",
    "print(a.get_shape())\n",
    "x = tf.constant([1,0,1])\n",
    "print(x.get_shape())\n",
    "sess = tf.InteractiveSession()\n",
    "print('Before expand dim: {}'.format(sess.run(x)))\n",
    "sess.close()\n",
    "x = tf.expand_dims(x,1)\n",
    "print(x.get_shape())\n",
    "\n",
    "b = tf.matmul(a,x)\n",
    "sess = tf.InteractiveSession()\n",
    "print('After expand dim: {}'.format(sess.run(x)))\n",
    "print('matmul result:\\n {}'.format(b.eval()))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:0\n",
      "c_1:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.Graph().as_default():\n",
    "    c1 = tf.constant(4,dtype=tf.float64,name='c')\n",
    "    c2 = tf.constant(4,dtype=tf.int32,name='c')\n",
    "print(c1.name)\n",
    "print(c2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:0\n",
      "prefix_name_2/c:0\n",
      "prefix_name_2/c_1:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nwe’ve grouped objects contained in variables c2 and c3 under\\nthe scope prefix_name, which shows up as a prefix in their names.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    c1 = tf.constant(4,dtype=tf.float64,name='c')\n",
    "with tf.name_scope(\"prefix_name\"):\n",
    "    c2 = tf.constant(4,dtype=tf.int32,name='c')\n",
    "    c3 = tf.constant(4,dtype=tf.float64,name='c')\n",
    "print(c1.name)\n",
    "print(c2.name)\n",
    "print(c3.name)\n",
    "\n",
    "'''\n",
    "we’ve grouped objects contained in variables c2 and c3 under\n",
    "the scope prefix_name, which shows up as a prefix in their names.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre run: \n",
      "<tf.Variable 'var_12:0' shape=(1, 5) dtype=float32_ref>\n",
      "pre run_1: \n",
      "<tf.Variable 'var1:0' shape=(1, 5) dtype=float32_ref>\n",
      "\n",
      "post run: \n",
      "[[-1.44122624 -0.66548991  0.88496304  0.5087021   0.04572999]]\n",
      "w1 <tf.Variable 'scope1_1/w1:0' shape=() dtype=int32_ref>\n",
      "w1_p <tf.Variable 'scope1_2/w1:0' shape=() dtype=int32_ref>\n",
      "w2 <tf.Variable 'scope1/w2:0' shape=() dtype=float32_ref>\n",
      "w2_p <tf.Variable 'scope1/w2:0' shape=() dtype=float32_ref>\n",
      "False True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntf.Variable()会自动处理冲突问题，如代码中所示，他会将scope1改为scope1_1。\\n\\n而tf.get_variable()会判断是否已经存在该name的变量，如果有，并且该变量空间的reuse=True,那么就可以直接共享之前的值;\\n如果没有，则重新创建。注意，如果你没有将reuse设为True，则会提示冲突发生。\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Variables  Variables can maintain a fixed state in the graph.\n",
    "#inputs. To reuse the same variable, we can use the tf.get_variable() function instead of tf.Variable().\n",
    "init_val = tf.random_normal((1,5),0,1)\n",
    "var = tf.Variable(init_val, name='var')\n",
    "print(\"pre run: \\n{}\".format(var))\n",
    "print(\"pre run_1: \\n{}\".format(var1))\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    post_var = sess.run(var)\n",
    "print(\"\\npost run: \\n{}\".format(post_var))\n",
    "\n",
    "\n",
    "\n",
    "with tf.variable_scope('scope1'):\n",
    "    w1 = tf.Variable(1, name='w1')\n",
    "    w2 = tf.get_variable(name='w2', initializer=2.)\n",
    "\n",
    "with tf.variable_scope('scope1', reuse=True):\n",
    "    w1_p = tf.Variable(1, name='w1')\n",
    "    w2_p = tf.get_variable(name='w2', initializer=3.)\n",
    "\n",
    "print('w1', w1)\n",
    "print('w1_p', w1_p)\n",
    "\n",
    "print('w2', w2)\n",
    "print('w2_p', w2_p)\n",
    "\n",
    "print(w1 is w1_p, w2 is w2_p)\n",
    "'''\n",
    "tf.Variable()会自动处理冲突问题，如代码中所示，他会将scope1改为scope1_1。\n",
    "\n",
    "而tf.get_variable()会判断是否已经存在该name的变量，如果有，并且该变量空间的reuse=True,那么就可以直接共享之前的值;\n",
    "如果没有，则重新创建。注意，如果你没有将reuse设为True，则会提示冲突发生。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs = 3.1141858100891113\n"
     ]
    }
   ],
   "source": [
    "#Placeholders\n",
    "#Placeholders can be thought of as empty\n",
    "#Variables that will be filled with data later on. We use them by first\n",
    "#constructing our graph and only when it is executed feeding them with the input data.\n",
    "import numpy as np\n",
    "x_data = np.random.randn(5,10)\n",
    "w_data = np.random.randn(10,1)\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(tf.float32,shape=(5,10))\n",
    "    w = tf.placeholder(tf.float32,shape=(10,1))\n",
    "    b = tf.fill((5,1),-1.)\n",
    "    xw = tf.matmul(x,w)\n",
    "    xwb = xw + b\n",
    "    s = tf.reduce_max(xwb) #axis：axis=none, 求全部元素的最大值；axis=0, 按列降维，求每列最大值；axis=1，按行降维，求每行最大值\n",
    "    with tf.Session() as sess:\n",
    "        outs = sess.run(s,feed_dict={x: x_data,w: w_data})\n",
    "print(\"outs = {}\".format(outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimization\n",
    "#Training to predict\n",
    "x = tf.placeholder(tf.float32,shape=[None,3])\n",
    "y_true = tf.placeholder(tf.float32,shape=None)\n",
    "w = tf.Variable([[0,0,0]],dtype=tf.float32,name='weights')\n",
    "b = tf.Variable(0,dtype=tf.float32,name='bias')\n",
    "\n",
    "y_pred = tf.matmul(w,tf.transpose(x)) + b\n",
    "\n",
    "#Defining a loss function\n",
    "\n",
    "#MSE loss = tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "#Cross entropy \n",
    "#loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
    "#loss = tf.reduce_mean(loss)\n",
    "\n",
    "#Opyimization\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[ 0.27842751,  0.48242491,  0.09638211]], dtype=float32), -0.20985541]\n",
      "5 [array([[ 0.29916903,  0.49749529,  0.09669954]], dtype=float32), -0.19915335]\n",
      "10 [array([[ 0.29916903,  0.49749532,  0.09669954]], dtype=float32), -0.19915335]\n"
     ]
    }
   ],
   "source": [
    "##Linear Regression example\n",
    "'''\n",
    "We create 2,000 samples of x, a vector with three features, take the inner product of\n",
    "each x sample with a set of weights w ([0.3, 0.5, 0.1]), and add a bias term b\n",
    "(–0.2) and Gaussian noise to the result\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "# === Create data and simulate results =====\n",
    "x_data = np.random.randn(2000,3) #2000 * 3 matrix\n",
    "w_real = [0.3,0.5,0.1]\n",
    "b_real = -0.2\n",
    "noise = np.random.randn(1,2000)*0.1\n",
    "y_data = np.matmul(w_real,x_data.T) + b_real + noise\n",
    "\n",
    "NUM_STEPS = 10\n",
    "g = tf.Graph()\n",
    "wb_ = []\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32,shape=[None,3])\n",
    "    y_true = tf.placeholder(tf.float32,shape=None)\n",
    "    with tf.name_scope('inference') as scope:\n",
    "        w = tf.Variable([[0,0,0]],dtype=tf.float32,name='weights')\n",
    "        b = tf.Variable(0,dtype=tf.float32,name='bias')\n",
    "        y_pred = tf.matmul(w,tf.transpose(x)) + b\n",
    "    with tf.name_scope('loss') as scope:\n",
    "        loss = tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "    with tf.name_scope('train') as scope:\n",
    "        learning_rate = 0.5\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train = optimizer.minimize(loss)\n",
    "    # Before starting, initialize the variables. We will 'run' this first.\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for step in range(NUM_STEPS):\n",
    "            sess.run(train,{x: x_data, y_true: y_data})\n",
    "            if (step % 5 == 0):\n",
    "                print(step, sess.run([w,b]))\n",
    "                wb_.append(sess.run([w,b]))\n",
    "        print(10, sess.run([w,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/logistic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[ 0.03617866,  0.05816923,  0.01194072]], dtype=float32), -0.021250073]\n",
      "5 [array([[ 0.16005653,  0.25803259,  0.05300526]], dtype=float32), -0.092755109]\n",
      "10 [array([[ 0.22623831,  0.36546242,  0.0751068 ]], dtype=float32), -0.12972547]\n",
      "15 [array([[ 0.2632215 ,  0.42583227,  0.08753879]], dtype=float32), -0.14972353]\n",
      "20 [array([[ 0.28451294,  0.46076441,  0.09473902]], dtype=float32), -0.16088426]\n",
      "25 [array([[ 0.29699197,  0.48133296,  0.09898274]], dtype=float32), -0.16723797]\n",
      "30 [array([[ 0.30438435,  0.49356878,  0.10150987]], dtype=float32), -0.17090146]\n",
      "35 [array([[ 0.3087914 ,  0.50089157,  0.10302392]], dtype=float32), -0.1730317]\n",
      "40 [array([[ 0.31142884,  0.50528949,  0.10393423]], dtype=float32), -0.17427762]\n",
      "45 [array([[ 0.31301093,  0.50793624,  0.1044827 ]], dtype=float32), -0.17500935]\n",
      "50 [array([[ 0.31380785,  0.50927317,  0.10476001]], dtype=float32), -0.17537142]\n"
     ]
    }
   ],
   "source": [
    "## logistic regression\n",
    "N = 20000\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "# === Create data and simulate results =====\n",
    "x_data = np.random.randn(N,3)\n",
    "w_real = [0.3,0.5,0.1]\n",
    "b_real = -0.2\n",
    "wxb = np.matmul(w_real,x_data.T) + b_real\n",
    "y_data_pre_noise = sigmoid(wxb)\n",
    "y_data = np.random.binomial(1,y_data_pre_noise)\n",
    "\n",
    "y_pred = tf.sigmoid(y_pred)\n",
    "#cross entropy\n",
    "#loss = y_true*tf.log(y_pred) - (1-y_true)*tf.log(1-y_pred)\n",
    "#loss = tf.reduce_mean(loss)\n",
    "NUM_STEPS = 50\n",
    "g = tf.Graph()\n",
    "wb_ = []\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32,shape=[None,3])\n",
    "    y_true = tf.placeholder(tf.float32,shape=None)\n",
    "    with tf.name_scope('inference') as scope:\n",
    "        w = tf.Variable([[0,0,0]],dtype=tf.float32,name='weights')\n",
    "        b = tf.Variable(0,dtype=tf.float32,name='bias')\n",
    "        y_pred = tf.matmul(w,tf.transpose(x)) + b\n",
    "    with tf.name_scope('loss') as scope:\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "    with tf.name_scope('train') as scope:\n",
    "        learning_rate = 0.5\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train = optimizer.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for step in range(NUM_STEPS):\n",
    "            sess.run(train,{x: x_data, y_true: y_data})\n",
    "            if (step % 5 == 0):\n",
    "                print(step, sess.run([w,b]))\n",
    "                wb_.append(sess.run([w,b]))\n",
    "        print(50, sess.run([w,b]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
