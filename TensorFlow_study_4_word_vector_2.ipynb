{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "two\n",
      "three\n",
      "four\n",
      "five\n",
      "six\n",
      "seven\n",
      "eight\n",
      "nine\n",
      "WARNING:tensorflow:From <ipython-input-14-5379359e292d>:145: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Accuracy at 0: 48.43750\n",
      "Accuracy at 100: 53.90625\n",
      "Accuracy at 200: 100.00000\n",
      "Accuracy at 300: 100.00000\n",
      "Accuracy at 400: 100.00000\n",
      "Accuracy at 500: 100.00000\n",
      "Accuracy at 600: 100.00000\n",
      "Accuracy at 700: 100.00000\n",
      "Accuracy at 800: 100.00000\n",
      "Accuracy at 900: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n"
     ]
    }
   ],
   "source": [
    "#Pretrained Word Embeddings\n",
    "#using glove\n",
    "\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "path_to_glove = \"data\"\n",
    "PRE_TRAINED = True\n",
    "GLOVE_SIZE = 300\n",
    "batch_size = 128\n",
    "embedding_dimension = 64\n",
    "num_classes = 2\n",
    "hidden_layer_size = 32\n",
    "times_steps = 6\n",
    "\n",
    "digit_to_word_map = {1:\"one\",2:\"two\", 3:\"three\", 4:\"four\", 5:\"five\",\n",
    "6:\"six\",7:\"seven\",8:\"eight\",9:\"nine\"}\n",
    "digit_to_word_map[0]=\"PAD_TOKEN\"\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2),\n",
    "    rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2),\n",
    "    rand_seq_len)\n",
    "    if rand_seq_len<6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints, [0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints, [0]*(6-rand_seq_len))\n",
    "    even_sentences.append(\" \".join([digit_to_word_map[r] for r in rand_odd_ints]))\n",
    "    odd_sentences.append(\" \".join([digit_to_word_map[r] for r in rand_even_ints]))\n",
    "    \n",
    "data = even_sentences+odd_sentences\n",
    "# Same seq lengths for even, odd sentences\n",
    "seqlens*=2\n",
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding\n",
    "    \n",
    "word2index_map ={}\n",
    "index=0\n",
    "for sent in data:\n",
    "    for word in sent.split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)\n",
    "\n",
    "def get_glove(path_to_glove,word2index_map):\n",
    "    embedding_weights = {}\n",
    "    count_all_words = 0\n",
    "    with open(r\"data\\glove.6B.300d.txt\",encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            vals = line.split()\n",
    "            word = str(vals[0])\n",
    "            if word in word2index_map:\n",
    "                print(word)\n",
    "                count_all_words+=1\n",
    "                coefs = np.asarray(vals[1:], dtype='float32')\n",
    "                coefs/=np.linalg.norm(coefs)\n",
    "                embedding_weights[word] = coefs\n",
    "            if count_all_words==vocabulary_size -1:\n",
    "                break\n",
    "    return embedding_weights\n",
    "\n",
    "word2embedding_dict = get_glove(path_to_glove,word2index_map)\n",
    "embedding_matrix = np.zeros((vocabulary_size ,GLOVE_SIZE))\n",
    "#for the PAD_TOKEN word, we set the corresponding vector to 0.\n",
    "for word,index in word2index_map.items():\n",
    "    if not word == \"PAD_TOKEN\":\n",
    "        word_embedding = word2embedding_dict[word]\n",
    "        embedding_matrix[index,:] = word_embedding\n",
    "\n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]\n",
    "\n",
    "def get_sentence_batch(batch_size,data_x,data_y,data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].split()] for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x,y,seqlens\n",
    "\n",
    "_inputs = tf.placeholder(tf.int32, shape=[batch_size,times_steps])\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, GLOVE_SIZE])\n",
    "_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "_seqlens = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "\n",
    "'''\n",
    "We set trainable=True to tell TensorFlow we want to update the\n",
    "values of the word vectors, by optimizing them for the task at hand.\n",
    "However, it is often useful to set trainable=False and not update these\n",
    "values; for example, when we do not have much labeled data or have reason\n",
    "to believe the word vectors are already “good” at capturing the patterns we\n",
    "are after.\n",
    "'''\n",
    "\n",
    "if PRE_TRAINED:\n",
    "    embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, GLOVE_SIZE]),trainable=True)\n",
    "    # If using pretrained embeddings, assign them to the embeddings variable\n",
    "    embedding_init = embeddings.assign(embedding_placeholder)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "else:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_dimension], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "\n",
    "#Bidirectional RNN and GRU Cells\n",
    "\n",
    "with tf.name_scope(\"biGRU\"):\n",
    "    with tf.variable_scope('forward'):\n",
    "        gru_fw_cell = tf.contrib.rnn.GRUCell(hidden_layer_size)\n",
    "        gru_fw_cell = tf.contrib.rnn.DropoutWrapper(gru_fw_cell)\n",
    "    with tf.variable_scope('backward'):\n",
    "        gru_bw_cell = tf.contrib.rnn.GRUCell(hidden_layer_size)\n",
    "        gru_bw_cell = tf.contrib.rnn.DropoutWrapper(gru_bw_cell)\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw=gru_fw_cell, cell_bw=gru_bw_cell, inputs=embed, sequence_length= _seqlens,\n",
    "                                                      dtype=tf.float32, scope=\"BiGRU\")\n",
    "\n",
    "#We concatenate the forward and backward state vectors by using tf.concat() along the suitable axis\n",
    "states = tf.concat(values=states, axis=1)\n",
    "\n",
    "weights = {'linear_layer': tf.Variable(tf.truncated_normal([2*hidden_layer_size, num_classes],mean=0,stddev=.01))} \n",
    "biases ={'linear_layer':tf.Variable(tf.truncated_normal([num_classes],mean=0,stddev=.01))}\n",
    "# extract the final state and use in a linear layer\n",
    "final_output = tf.matmul(states,\n",
    "weights[\"linear_layer\"]) + biases[\"linear_layer\"]\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits=final_output,labels=_labels)\n",
    "cross_entropy = tf.reduce_mean(softmax)\n",
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(_labels,1),tf.argmax(final_output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction,tf.float32)))*100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(embedding_init, feed_dict= {embedding_placeholder: embedding_matrix})\n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch,seqlen_batch = get_sentence_batch(batch_size,train_x,train_y, train_seqlens)\n",
    "        sess.run(train_step,feed_dict={_inputs:x_batch, _labels:y_batch,_seqlens:seqlen_batch})\n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={_inputs:x_batch,_labels:y_batch,_seqlens:seqlen_batch})\n",
    "            print(\"Accuracy at %d: %.5f\" % (step, acc))\n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test,seqlen_test = get_sentence_batch(batch_size,test_x,test_y,test_seqlens)\n",
    "        batch_pred,batch_acc = sess.run([tf.argmax(final_output,1),accuracy],\n",
    "                                        feed_dict={_inputs:x_test,_labels:y_test, _seqlens:seqlen_test})\n",
    "        print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))\n",
    "        print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
