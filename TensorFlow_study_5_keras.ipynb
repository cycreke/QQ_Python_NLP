{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import keras\n",
    "\n",
    "#Sequential model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, input_dim=784))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# or equal \n",
    "model = Sequential([Dense(64, input_shape=(784,),activation='softmax')])\n",
    "\n",
    "'''\n",
    "A dense layer is a fully connected layer. The first argument denotes the\n",
    "number of output units, and the input shape is the shape of the input\n",
    "'''\n",
    "\n",
    "#learning configurations\n",
    "'''\n",
    "the loss function, the optimizer, and another metric function that is used to\n",
    "judge the performance of your model (not used as the actual loss when training the model)\n",
    "'''\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#set the optimizer\n",
    "optimizer=keras.optimizers.SGD(lr=0.02, momentum=0.8, nesterov=True))\n",
    "\n",
    "#callbacks用于指定在每个epoch开始和结束的时候进行哪种特定操作\n",
    "#EarlyStopping则是用于提前停止训练的callbacks。具体地，可以达到当训练集上的loss不在减小（即减小的程度小于某个阈值）的时候停止继续训练。\n",
    "#https://blog.csdn.net/silent56_th/article/details/72845912\n",
    "#In Keras we can specify the minimum change to be monitored (min_delta), the number of no-improvement epochs to stop after (patience),\n",
    "# and the direction of wanted change (mode).\n",
    "\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=64, callbacks=[TensorBoard(log_dir='\\models\\autoencoder',) early_stop])\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "classes = model.predict(x_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functional model\n",
    "\n",
    "'''\n",
    "The main practical difference between the functional model and the\n",
    "sequential model is that here we first define our input and output, and only\n",
    "then instantiate the model\n",
    "'''\n",
    "\n",
    "#create an input Tensor\n",
    "inputs = Input(shape=(784,))\n",
    "\n",
    "#define model\n",
    "x = Dense(64, activation='relu')(inputs) #the layers act as functions\n",
    "x = Dense(32, activation='relu')(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=64)\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "classes = model.predict(x_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/autoencoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inp_img = tf.keras.layers.Input(shape=(32, 32, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 1s 165us/step - loss: 0.6282 - val_loss: 0.5768\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 1s 117us/step - loss: 0.5732 - val_loss: 0.5664\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 1s 116us/step - loss: 0.5619 - val_loss: 0.5637\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 1s 116us/step - loss: 0.5567 - val_loss: 0.5496\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 1s 116us/step - loss: 0.5528 - val_loss: 0.5472\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 1s 109us/step - loss: 0.5499 - val_loss: 0.5478\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 1s 122us/step - loss: 0.5476 - val_loss: 0.5427\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 1s 116us/step - loss: 0.5465 - val_loss: 0.5426\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 1s 122us/step - loss: 0.5444 - val_loss: 0.5413\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 1s 118us/step - loss: 0.5438 - val_loss: 0.5404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x17d326b85c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Autoencoders 自编码器\n",
    "#https://blog.csdn.net/lwq1026/article/details/78581649\n",
    "#https://blog.csdn.net/touch_dream/article/details/77500817\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train[np.where(y_train==1)[0],:,:,:]\n",
    "x_test = x_test[np.where(y_test==1)[0],:,:,:]\n",
    "\n",
    "#normalizing it to a range between [0,1].\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "#add some Gaussian noise\n",
    "x_train_n = x_train + 0.5 * np.random.normal(loc=0.0, scale=0.4, size=x_train.shape)\n",
    "x_test_n = x_test + 0.5 * np.random.normal(loc=0.0, scale=0.4, size=x_test.shape)\n",
    "\n",
    "#clip这个函数将数组中的元素限制在a_min, a_max之间，大于a_max的就使得它等于 a_max，小于a_min,的就使得它等于a_min\n",
    "x_train_n = np.clip(x_train_n, 0., 1.)\n",
    "x_test_n = np.clip(x_test_n, 0., 1.)\n",
    "\n",
    "inp_img = tf.keras.Input(shape=(32, 32, 3))\n",
    "#first argument is the number of filters (and thus the number of output images), and the second is the size of each filter\n",
    "img = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inp_img)\n",
    "img = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(img)\n",
    "img = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(img)\n",
    "img = tf.keras.layers.UpSampling2D((2, 2))(img)\n",
    "decoded = tf.keras.layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(img) #重新卷积到 3 channels\n",
    "\n",
    "autoencoder = tf.keras.Model(inp_img, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='logs\\keras', histogram_freq=0, write_graph=True, write_images=True)\n",
    "model_saver = tf.keras.callbacks.ModelCheckpoint(filepath='logs\\keras\\checkpoint-{epoch:02d}.hdf5',verbose=0, period=2)\n",
    "autoencoder.fit(x_train_n, x_train,epochs=10,batch_size=64,shuffle=True,validation_data=(x_test_n, x_test),\n",
    "                callbacks=[tensorboard, model_saver])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
